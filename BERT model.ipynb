{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "tough-sweden",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from cdqa.utils.converters import pdf_converter\n",
    "from cdqa.pipeline import QAPipeline\n",
    "from cdqa.utils.download import download_model\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "guided-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mytext\", 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "accepted-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['paragraphs'], data=[[lines]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "rolled-frederick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The pre-trained model can then be fine-tuned ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paragraphs\n",
       "0  [The pre-trained model can then be fine-tuned ..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "medieval-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = 'Passage1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "swedish-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_http_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "faced-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading trained model...\n",
      "bert_qa.joblib already downloaded\n"
     ]
    }
   ],
   "source": [
    "download_model(model = 'bert-squad_1.1', dir='./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cardiac-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘docs’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "brilliant-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pdf_converter(directory_path='MyFile.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bottom-keyboard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The pre-trained model can then be fine-tuned ...</td>\n",
       "      <td>Passage1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paragraphs     title\n",
       "0  [The pre-trained model can then be fine-tuned ...  Passage1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "practical-floor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "nonprofit-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdqa_pipeline = QAPipeline(rander='./models/bert_qa.joblib',max_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "moderate-proceeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QAPipeline(reader=BertQA(adam_epsilon=1e-08, bert_model='bert-base-uncased',\n",
       "                         do_lower_case=True, fp16=False,\n",
       "                         gradient_accumulation_steps=1, learning_rate=5e-05,\n",
       "                         local_rank=-1, loss_scale=0, max_answer_length=30,\n",
       "                         n_best_size=20, no_cuda=False,\n",
       "                         null_score_diff_threshold=0.0, num_train_epochs=3.0,\n",
       "                         output_dir=None, predict_batch_size=8, seed=42,\n",
       "                         server_ip='', server_po..._size=32,\n",
       "                         verbose_logging=False, version_2_with_negative=False,\n",
       "                         warmup_proportion=0.1, warmup_steps=0),\n",
       "           retrieve_by_doc=False,\n",
       "           retriever=BM25Retriever(b=0.75, floor=None, k1=2.0, lowercase=True,\n",
       "                                   max_df=10, min_df=2, ngram_range=(1, 2),\n",
       "                                   preprocessor=None, stop_words='english',\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, top_n=20, verbose=False,\n",
       "                                   vocabulary=None))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdqa_pipeline.fit_retriever(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "agricultural-sessions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/bert_qa_custom.joblib']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cdqa_pipeline, './models/bert_qa_custom.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "wooden-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QAPipeline(reader=BertQA(adam_epsilon=1e-08, bert_model='bert-base-uncased',\n",
       "                         do_lower_case=True, fp16=False,\n",
       "                         gradient_accumulation_steps=1, learning_rate=5e-05,\n",
       "                         local_rank=-1, loss_scale=0, max_answer_length=30,\n",
       "                         n_best_size=20, no_cuda=False,\n",
       "                         null_score_diff_threshold=0.0, num_train_epochs=3.0,\n",
       "                         output_dir=None, predict_batch_size=8, seed=42,\n",
       "                         server_ip='', server_po..._size=32,\n",
       "                         verbose_logging=False, version_2_with_negative=False,\n",
       "                         warmup_proportion=0.1, warmup_steps=0),\n",
       "           retrieve_by_doc=False,\n",
       "           retriever=BM25Retriever(b=0.75, floor=None, k1=2.0, lowercase=True,\n",
       "                                   max_df=10, min_df=1, ngram_range=(1, 2),\n",
       "                                   preprocessor=None, stop_words='english',\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, top_n=20, verbose=False,\n",
       "                                   vocabulary=None))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdqa_pipeline=joblib.load('./models/bert_qa_custom.joblib')\n",
    "cdqa_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "sharp-north",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('upon recent work in pre-training contextual representations — including Semi-supervised',\n",
       " 'Passage1',\n",
       " 'BERT builds upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).\\n',\n",
       " 1.151718433158566)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What Makes BERT Different?'\n",
    "prediction = cdqa_pipeline.predict(query)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "civic-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is Bidirectionality'\n",
    "prediction = cdqa_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "centered-calvin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Strength of Bidirection',\n",
       " 'Passage1',\n",
       " 'The Strength of Bidirectionality\\n',\n",
       " 1.7395531392772847)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "considered-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How much is increase in operating cash flow?'\n",
    "prediction = cdqa_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "complex-determination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('anyone in the world can train their own state-of-the-art question',\n",
       " 'Passage1',\n",
       " 'This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. \\n',\n",
       " 1.0307213723659516)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "offensive-mixer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('anyone in the world can train their own state-of-the-art question',\n",
       " 'Passage1',\n",
       " 'This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. \\n',\n",
       " 1.0362496912479402)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What is latest earnings per share?'\n",
    "cdqa_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "overall-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How many jobs are created in 2020?'\n",
    "prediction = cdqa_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "supreme-vertical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: How many jobs are created in 2020?\n",
      "answer: anyone in the world can train their own state-of-the-art\n",
      "title: Passage1\n",
      "paragraph: This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('query: {}'.format(query))\n",
    "print('answer: {}'.format(prediction[0]))\n",
    "print('title: {}'.format(prediction[1]))\n",
    "print('paragraph: {}'.format(prediction[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "contemporary-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How many full time employees are on Amazon roll?'\n",
    "prediction = cdqa_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dominant-device",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: How many full time employees are on Amazon roll?\n",
      "answer: anyone in the world can train their own state-of-the-art\n",
      "title: Passage1\n",
      "paragraph: This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('query: {}'.format(query))\n",
    "print('answer: {}'.format(prediction[0]))\n",
    "print('title: {}'.format(prediction[1]))\n",
    "print('paragraph: {}'.format(prediction[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "continuous-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'General Availability of which AWS services were announced?'\n",
    "prediction = cdqa_pipeline.predict(query, n_predictions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "electoral-albert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anyone in the world can train their own state-of-the-art',\n",
       "  'Passage1',\n",
       "  'This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. \\n',\n",
       "  1.0638655990362167),\n",
       " ('(in this case,',\n",
       "  'Passage1',\n",
       "  'BERT builds upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).\\n',\n",
       "  0.817771278321743),\n",
       " ('in substantial accuracy improvements compared to training on these',\n",
       "  'Passage1',\n",
       "  'The pre-trained model can then be fine-tuned on small-data NLP tasks like question answering and sentiment analysis, resulting in substantial accuracy improvements compared to training on these datasets from scratch. \\n',\n",
       "  0.7028021827340126),\n",
       " ('on the previous words in the sentence',\n",
       "  'Passage1',\n",
       "  'If bidirectionality is so powerful, why hasn’t it been done before? To understand why, consider that unidirectional models are efficiently trained by predicting each word conditioned on the previous words in the sentence',\n",
       "  0.6706782922148705),\n",
       " ('What', 'Passage1', 'What Makes BERT Different?\\n', 0.4428860768675804)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "invisible-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the impact of COVID on business?'\n",
    "prediction = cdqa_pipeline.predict(query, n_predictions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "every-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: What is the impact of COVID on business?\n",
      "answer: ('anyone in the world can train their own state-of-the-', 'Passage1', 'This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. \\n', 1.0991828441619873)\n",
      "title: ('(in this case,', 'Passage1', 'BERT builds upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).\\n', 0.8517892956733704)\n",
      "paragraph: ('-tuned on small-data NLP tasks like question', 'Passage1', 'The pre-trained model can then be fine-tuned on small-data NLP tasks like question answering and sentiment analysis, resulting in substantial accuracy improvements compared to training on these datasets from scratch. \\n', 0.7518077045679092)\n"
     ]
    }
   ],
   "source": [
    "print('query: {}'.format(query))\n",
    "print('answer: {}'.format(prediction[0]))\n",
    "print('title: {}'.format(prediction[1]))\n",
    "print('paragraph: {}'.format(prediction[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-upper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-eagle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
